{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Parquet Operations in Python: Schema Evolution, Metadata, and Performance Tuning\n",
    "\n",
    "This advanced tutorial covers sophisticated Parquet operations including schema management, metadata handling, performance optimization, and integration with big data frameworks.\n",
    "\n",
    "## Table of Contents\n",
    "1. Advanced Schema Management\n",
    "2. Working with Metadata\n",
    "3. Schema Evolution Strategies\n",
    "4. Advanced Filtering and Projection\n",
    "5. Memory-Efficient Operations\n",
    "6. Integration with Dask and PySpark\n",
    "7. Performance Optimization Techniques\n",
    "8. Data Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Schema Management\n",
    "\n",
    "Understanding and managing Parquet schemas is crucial for data consistency and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex dataset with various data types\n",
    "np.random.seed(42)\n",
    "n_records = 10000\n",
    "\n",
    "# Complex data including nested structures\n",
    "complex_data = {\n",
    "    'id': range(n_records),\n",
    "    'timestamp': pd.date_range('2023-01-01', periods=n_records, freq='5min'),\n",
    "    'user_id': np.random.randint(1000, 5000, n_records),\n",
    "    'event_type': np.random.choice(['click', 'view', 'purchase', 'share'], n_records),\n",
    "    'value': np.random.uniform(0.1, 1000, n_records),\n",
    "    'metadata': [{'source': 'web', 'version': f'v{i%3+1}'} for i in range(n_records)],\n",
    "    'tags': [['tag1', 'tag2'] if i % 2 == 0 else ['tag3'] for i in range(n_records)],\n",
    "    'is_valid': np.random.choice([True, False], n_records, p=[0.95, 0.05])\n",
    "}\n",
    "\n",
    "df_complex = pd.DataFrame(complex_data)\n",
    "print(\"Complex dataset created:\")\n",
    "print(df_complex.info())\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "df_complex.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom schema with specific types and metadata\n",
    "custom_schema = pa.schema([\n",
    "    pa.field('id', pa.int64(), nullable=False),\n",
    "    pa.field('timestamp', pa.timestamp('ns'), nullable=False),\n",
    "    pa.field('user_id', pa.int32()),  # Downcast from int64 to int32\n",
    "    pa.field('event_type', pa.dictionary(pa.int8(), pa.string())),  # Dictionary encoding\n",
    "    pa.field('value', pa.float32()),  # Downcast from float64 to float32\n",
    "    pa.field('metadata', pa.map_(pa.string(), pa.string())),  # Map type for key-value pairs\n",
    "    pa.field('tags', pa.list_(pa.string())),  # List type\n",
    "    pa.field('is_valid', pa.bool_())\n",
    "], metadata={'created_by': 'Advanced Parquet Tutorial', 'version': '1.0'})\n",
    "\n",
    "# Convert DataFrame to Table with custom schema\n",
    "# Note: We need to handle the metadata column specially\n",
    "df_for_arrow = df_complex.copy()\n",
    "df_for_arrow['metadata'] = df_for_arrow['metadata'].apply(lambda x: list(x.items()))\n",
    "\n",
    "table = pa.Table.from_pandas(df_for_arrow, schema=custom_schema, preserve_index=False)\n",
    "\n",
    "# Write with custom schema\n",
    "pq.write_table(table, 'advanced_schema.parquet')\n",
    "print(\"✅ Parquet file written with custom schema\")\n",
    "print(f\"\\nSchema:\\n{table.schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working with Metadata\n",
    "\n",
    "Parquet files can store custom metadata at both file and column levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metadata to table\n",
    "metadata = {\n",
    "    'department': 'Data Engineering',\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'data_source': 'Event Stream',\n",
    "    'processing_version': '2.1.0',\n",
    "    'row_count': str(len(df_complex))\n",
    "}\n",
    "\n",
    "# Create table with metadata\n",
    "table_with_metadata = table.replace_schema_metadata(metadata)\n",
    "\n",
    "# Write table with metadata\n",
    "pq.write_table(table_with_metadata, 'data_with_metadata.parquet')\n",
    "\n",
    "# Read and display metadata\n",
    "parquet_file = pq.ParquetFile('data_with_metadata.parquet')\n",
    "print(\"📋 File Metadata:\")\n",
    "print(json.dumps(parquet_file.schema_arrow.metadata, indent=2))\n",
    "\n",
    "# Column-level metadata\n",
    "print(\"\\n📊 Column Metadata:\")\n",
    "for field in parquet_file.schema_arrow:\n",
    "    if field.metadata:\n",
    "        print(f\"{field.name}: {field.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file statistics and metadata\n",
    "metadata = parquet_file.metadata\n",
    "print(f\"📈 File Statistics:\")\n",
    "print(f\"Number of rows: {metadata.num_rows:,}\")\n",
    "print(f\"Number of row groups: {metadata.num_row_groups}\")\n",
    "print(f\"Format version: {metadata.format_version}\")\n",
    "print(f\"Created by: {metadata.created_by}\")\n",
    "\n",
    "# Row group statistics\n",
    "for i in range(metadata.num_row_groups):\n",
    "    rg = metadata.row_group(i)\n",
    "    print(f\"\\n📦 Row Group {i}:\")\n",
    "    print(f\"  Rows: {rg.num_rows:,}\")\n",
    "    print(f\"  Total byte size: {rg.total_byte_size:,}\")\n",
    "    print(f\"  Columns: {rg.num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schema Evolution Strategies\n",
    "\n",
    "Handle schema changes over time without breaking existing data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original schema (v1)\n",
    "data_v1 = {\n",
    "    'id': range(1000),\n",
    "    'name': [f'User_{i}' for i in range(1000)],\n",
    "    'score': np.random.randint(0, 100, 1000)\n",
    "}\n",
    "df_v1 = pd.DataFrame(data_v1)\n",
    "df_v1.to_parquet('schema_v1.parquet')\n",
    "print(\"Schema v1:\")\n",
    "print(df_v1.dtypes)\n",
    "\n",
    "# Evolved schema (v2) - Added new columns\n",
    "data_v2 = {\n",
    "    'id': range(1000, 2000),\n",
    "    'name': [f'User_{i}' for i in range(1000, 2000)],\n",
    "    'score': np.random.randint(0, 100, 1000),\n",
    "    'created_at': pd.date_range('2024-01-01', periods=1000, freq='H'),  # New column\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 1000)  # New column\n",
    "}\n",
    "df_v2 = pd.DataFrame(data_v2)\n",
    "df_v2.to_parquet('schema_v2.parquet')\n",
    "print(\"\\nSchema v2 (with new columns):\")\n",
    "print(df_v2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle schema evolution when reading\n",
    "def read_with_schema_evolution(file_paths, fill_missing=True):\n",
    "    \"\"\"\n",
    "    Read multiple Parquet files with potentially different schemas\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    \n",
    "    for path in file_paths:\n",
    "        df = pd.read_parquet(path)\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    # Combine with schema alignment\n",
    "    if fill_missing:\n",
    "        # Get all columns across all dataframes\n",
    "        all_columns = set()\n",
    "        for df in dataframes:\n",
    "            all_columns.update(df.columns)\n",
    "        \n",
    "        # Add missing columns with default values\n",
    "        aligned_dfs = []\n",
    "        for df in dataframes:\n",
    "            for col in all_columns:\n",
    "                if col not in df.columns:\n",
    "                    # Add default values based on expected type\n",
    "                    if col == 'created_at':\n",
    "                        df[col] = pd.NaT\n",
    "                    elif col == 'category':\n",
    "                        df[col] = 'Unknown'\n",
    "                    else:\n",
    "                        df[col] = None\n",
    "            aligned_dfs.append(df[sorted(all_columns)])\n",
    "        \n",
    "        return pd.concat(aligned_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.concat(dataframes, ignore_index=True, sort=True)\n",
    "\n",
    "# Read files with different schemas\n",
    "combined_df = read_with_schema_evolution(['schema_v1.parquet', 'schema_v2.parquet'])\n",
    "print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "print(f\"\\nColumns: {combined_df.columns.tolist()}\")\n",
    "print(f\"\\nSample of combined data:\")\n",
    "combined_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Filtering and Projection Pushdown\n",
    "\n",
    "Leverage Parquet's columnar format for efficient data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large dataset for performance testing\n",
    "n_large = 1000000\n",
    "large_data = {\n",
    "    'id': range(n_large),\n",
    "    'timestamp': pd.date_range('2023-01-01', periods=n_large, freq='1min'),\n",
    "    'sensor_id': np.random.randint(1, 100, n_large),\n",
    "    'temperature': np.random.normal(25, 5, n_large),\n",
    "    'humidity': np.random.normal(60, 10, n_large),\n",
    "    'pressure': np.random.normal(1013, 20, n_large),\n",
    "    'location': np.random.choice(['North', 'South', 'East', 'West'], n_large),\n",
    "    'status': np.random.choice(['OK', 'WARNING', 'ERROR'], n_large, p=[0.9, 0.08, 0.02])\n",
    "}\n",
    "df_large = pd.DataFrame(large_data)\n",
    "\n",
    "# Write with row groups for better filtering\n",
    "df_large.to_parquet('sensor_data.parquet', \n",
    "                   row_group_size=50000,  # 50k rows per group\n",
    "                   engine='pyarrow')\n",
    "\n",
    "print(f\"✅ Large dataset created: {len(df_large):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced filtering with PyArrow compute functions\n",
    "parquet_file = pq.ParquetFile('sensor_data.parquet')\n",
    "\n",
    "# Method 1: Filter using PyArrow expressions\n",
    "start_time = time.time()\n",
    "filters = [\n",
    "    ('status', '==', 'ERROR'),\n",
    "    ('temperature', '>', 30)\n",
    "]\n",
    "filtered_table = pq.read_table('sensor_data.parquet', \n",
    "                              filters=filters,\n",
    "                              columns=['id', 'timestamp', 'sensor_id', 'temperature', 'status'])\n",
    "filter_time = time.time() - start_time\n",
    "\n",
    "print(f\"⚡ Filtered read: {len(filtered_table):,} rows in {filter_time:.3f} seconds\")\n",
    "\n",
    "# Method 2: Read all and filter in memory (for comparison)\n",
    "start_time = time.time()\n",
    "df_all = pd.read_parquet('sensor_data.parquet')\n",
    "df_filtered_pandas = df_all[(df_all['status'] == 'ERROR') & (df_all['temperature'] > 30)]\n",
    "pandas_time = time.time() - start_time\n",
    "\n",
    "print(f\"🐼 Pandas filter: {len(df_filtered_pandas):,} rows in {pandas_time:.3f} seconds\")\n",
    "print(f\"\\n🚀 Parquet filtering is {pandas_time/filter_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex filtering with PyArrow compute\n",
    "table = pq.read_table('sensor_data.parquet')\n",
    "\n",
    "# Create complex filter expressions\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# Filter: (temperature > 30 OR humidity < 50) AND status != 'OK'\n",
    "temp_filter = pc.greater(table['temperature'], 30)\n",
    "humidity_filter = pc.less(table['humidity'], 50)\n",
    "status_filter = pc.not_equal(table['status'], 'OK')\n",
    "\n",
    "# Combine filters\n",
    "complex_filter = pc.and_(pc.or_(temp_filter, humidity_filter), status_filter)\n",
    "\n",
    "# Apply filter\n",
    "filtered_table = table.filter(complex_filter)\n",
    "print(f\"Complex filter result: {len(filtered_table):,} rows\")\n",
    "\n",
    "# Convert to pandas for analysis\n",
    "df_complex_filtered = filtered_table.to_pandas()\n",
    "print(f\"\\nFiltered data statistics:\")\n",
    "print(df_complex_filtered[['temperature', 'humidity', 'status']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory-Efficient Operations\n",
    "\n",
    "Handle large datasets that don't fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process large file in batches\n",
    "def process_parquet_in_batches(file_path, batch_size=10000, process_func=None):\n",
    "    \"\"\"\n",
    "    Process large Parquet file in batches to manage memory usage\n",
    "    \"\"\"\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    results = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Process each batch\n",
    "    for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "        df_batch = batch.to_pandas()\n",
    "        total_rows += len(df_batch)\n",
    "        \n",
    "        if process_func:\n",
    "            result = process_func(df_batch)\n",
    "            results.append(result)\n",
    "        \n",
    "        # Show progress\n",
    "        if total_rows % 100000 == 0:\n",
    "            print(f\"Processed {total_rows:,} rows...\")\n",
    "    \n",
    "    return results, total_rows\n",
    "\n",
    "# Example: Calculate statistics per batch\n",
    "def calculate_batch_stats(df):\n",
    "    return {\n",
    "        'mean_temp': df['temperature'].mean(),\n",
    "        'error_count': (df['status'] == 'ERROR').sum(),\n",
    "        'row_count': len(df)\n",
    "    }\n",
    "\n",
    "# Process in batches\n",
    "batch_results, total = process_parquet_in_batches(\n",
    "    'sensor_data.parquet',\n",
    "    batch_size=50000,\n",
    "    process_func=calculate_batch_stats\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Processed {total:,} rows in {len(batch_results)} batches\")\n",
    "\n",
    "# Aggregate results\n",
    "total_errors = sum(r['error_count'] for r in batch_results)\n",
    "avg_temp = np.average([r['mean_temp'] for r in batch_results], \n",
    "                     weights=[r['row_count'] for r in batch_results])\n",
    "\n",
    "print(f\"\\n📊 Aggregate Statistics:\")\n",
    "print(f\"Average temperature: {avg_temp:.2f}°C\")\n",
    "print(f\"Total errors: {total_errors:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory mapping for random access\n",
    "# This is useful for accessing specific parts of the file without loading everything\n",
    "\n",
    "# Get specific row groups based on condition\n",
    "def get_row_groups_by_stats(file_path, column, min_value=None, max_value=None):\n",
    "    \"\"\"\n",
    "    Get row groups that might contain values in specified range\n",
    "    using Parquet statistics\n",
    "    \"\"\"\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    relevant_row_groups = []\n",
    "    \n",
    "    for i in range(parquet_file.num_row_groups):\n",
    "        # Get row group metadata\n",
    "        rg_meta = parquet_file.metadata.row_group(i)\n",
    "        \n",
    "        # Find column index\n",
    "        col_idx = None\n",
    "        for j in range(rg_meta.num_columns):\n",
    "            col = rg_meta.column(j)\n",
    "            if col.path_in_schema == column:\n",
    "                col_idx = j\n",
    "                break\n",
    "        \n",
    "        if col_idx is not None:\n",
    "            col_meta = rg_meta.column(col_idx)\n",
    "            if col_meta.statistics:\n",
    "                stats = col_meta.statistics\n",
    "                # Check if row group might contain values in range\n",
    "                if ((min_value is None or stats.max >= min_value) and \n",
    "                    (max_value is None or stats.min <= max_value)):\n",
    "                    relevant_row_groups.append(i)\n",
    "    \n",
    "    return relevant_row_groups\n",
    "\n",
    "# Find row groups with high temperatures\n",
    "hot_row_groups = get_row_groups_by_stats('sensor_data.parquet', \n",
    "                                         'temperature', \n",
    "                                         min_value=35)\n",
    "\n",
    "print(f\"Row groups potentially containing temperature > 35°C: {hot_row_groups}\")\n",
    "\n",
    "# Read only specific row groups\n",
    "if hot_row_groups:\n",
    "    parquet_file = pq.ParquetFile('sensor_data.parquet')\n",
    "    hot_data = parquet_file.read_row_groups(hot_row_groups)\n",
    "    df_hot = hot_data.to_pandas()\n",
    "    actual_hot = df_hot[df_hot['temperature'] > 35]\n",
    "    print(f\"Actual rows with temperature > 35°C: {len(actual_hot):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with Dask for Distributed Processing\n",
    "\n",
    "Dask enables parallel processing of large Parquet datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Install dask with: pip install dask[complete]\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    \n",
    "    # Read Parquet with Dask\n",
    "    ddf = dd.read_parquet('sensor_data.parquet', \n",
    "                         engine='pyarrow',\n",
    "                         index=False)\n",
    "    \n",
    "    print(f\"Dask DataFrame created with {ddf.npartitions} partitions\")\n",
    "    \n",
    "    # Perform operations lazily\n",
    "    result = ddf.groupby('location')['temperature'].agg(['mean', 'min', 'max', 'count'])\n",
    "    \n",
    "    # Compute results\n",
    "    computed_result = result.compute()\n",
    "    print(\"\\nTemperature statistics by location:\")\n",
    "    print(computed_result)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Dask not installed. Install with: pip install dask[complete]\")\n",
    "    print(\"Skipping Dask examples...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different optimization strategies\n",
    "optimization_results = []\n",
    "\n",
    "# 1. Default write\n",
    "start = time.time()\n",
    "df_large.to_parquet('test_default.parquet')\n",
    "default_time = time.time() - start\n",
    "default_size = os.path.getsize('test_default.parquet') / (1024**2)\n",
    "optimization_results.append({\n",
    "    'method': 'Default',\n",
    "    'write_time': default_time,\n",
    "    'file_size_mb': default_size\n",
    "})\n",
    "\n",
    "# 2. Dictionary encoding for categorical columns\n",
    "df_optimized = df_large.copy()\n",
    "df_optimized['location'] = df_optimized['location'].astype('category')\n",
    "df_optimized['status'] = df_optimized['status'].astype('category')\n",
    "\n",
    "start = time.time()\n",
    "df_optimized.to_parquet('test_dictionary.parquet')\n",
    "dict_time = time.time() - start\n",
    "dict_size = os.path.getsize('test_dictionary.parquet') / (1024**2)\n",
    "optimization_results.append({\n",
    "    'method': 'Dictionary Encoding',\n",
    "    'write_time': dict_time,\n",
    "    'file_size_mb': dict_size\n",
    "})\n",
    "\n",
    "# 3. Type optimization\n",
    "df_type_optimized = df_large.copy()\n",
    "df_type_optimized['sensor_id'] = df_type_optimized['sensor_id'].astype('int16')  # was int64\n",
    "df_type_optimized['temperature'] = df_type_optimized['temperature'].astype('float32')  # was float64\n",
    "df_type_optimized['humidity'] = df_type_optimized['humidity'].astype('float32')\n",
    "df_type_optimized['pressure'] = df_type_optimized['pressure'].astype('float32')\n",
    "\n",
    "start = time.time()\n",
    "df_type_optimized.to_parquet('test_optimized_types.parquet')\n",
    "type_time = time.time() - start\n",
    "type_size = os.path.getsize('test_optimized_types.parquet') / (1024**2)\n",
    "optimization_results.append({\n",
    "    'method': 'Optimized Types',\n",
    "    'write_time': type_time,\n",
    "    'file_size_mb': type_size\n",
    "})\n",
    "\n",
    "# Results\n",
    "opt_df = pd.DataFrame(optimization_results)\n",
    "opt_df['size_reduction'] = (1 - opt_df['file_size_mb'] / default_size) * 100\n",
    "print(\"Optimization Results:\")\n",
    "opt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_parquet_file(file_path):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of Parquet file\n",
    "    \"\"\"\n",
    "    results = {'file': file_path, 'checks': {}}\n",
    "    \n",
    "    try:\n",
    "        # Basic file checks\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        results['checks']['file_readable'] = True\n",
    "        results['checks']['num_rows'] = parquet_file.metadata.num_rows\n",
    "        results['checks']['num_row_groups'] = parquet_file.num_row_groups\n",
    "        \n",
    "        # Schema validation\n",
    "        schema = parquet_file.schema_arrow\n",
    "        results['checks']['num_columns'] = len(schema)\n",
    "        results['checks']['column_names'] = schema.names\n",
    "        \n",
    "        # Check for corrupted data\n",
    "        corrupted_row_groups = []\n",
    "        for i in range(parquet_file.num_row_groups):\n",
    "            try:\n",
    "                _ = parquet_file.read_row_group(i)\n",
    "            except Exception as e:\n",
    "                corrupted_row_groups.append(i)\n",
    "        \n",
    "        results['checks']['corrupted_row_groups'] = corrupted_row_groups\n",
    "        results['checks']['data_integrity'] = len(corrupted_row_groups) == 0\n",
    "        \n",
    "        # Statistics availability\n",
    "        stats_available = []\n",
    "        for i in range(parquet_file.metadata.row_group(0).num_columns):\n",
    "            col = parquet_file.metadata.row_group(0).column(i)\n",
    "            if col.statistics:\n",
    "                stats_available.append(col.path_in_schema)\n",
    "        \n",
    "        results['checks']['statistics_available'] = stats_available\n",
    "        \n",
    "        # Compression info\n",
    "        compressions = set()\n",
    "        for i in range(parquet_file.num_row_groups):\n",
    "            rg = parquet_file.metadata.row_group(i)\n",
    "            for j in range(rg.num_columns):\n",
    "                col = rg.column(j)\n",
    "                compressions.add(str(col.compression))\n",
    "        \n",
    "        results['checks']['compression_types'] = list(compressions)\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['checks']['file_readable'] = False\n",
    "        results['checks']['error'] = str(e)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Validate our files\n",
    "validation_results = validate_parquet_file('sensor_data.parquet')\n",
    "print(\"Parquet File Validation Results:\")\n",
    "print(json.dumps(validation_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove temporary files\n",
    "import glob\n",
    "\n",
    "files_to_remove = glob.glob('*.parquet')\n",
    "for file in files_to_remove:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "print(f\"✅ Removed {len(files_to_remove)} temporary files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Schema Management**\n",
    "   - Define explicit schemas for consistency\n",
    "   - Use dictionary encoding for categorical data\n",
    "   - Plan for schema evolution from the start\n",
    "\n",
    "2. **Performance Optimization**\n",
    "   - Use appropriate data types (int32 vs int64, float32 vs float64)\n",
    "   - Enable statistics for better query planning\n",
    "   - Partition large datasets by commonly filtered columns\n",
    "\n",
    "3. **Memory Efficiency**\n",
    "   - Process large files in batches\n",
    "   - Use row group statistics for selective reading\n",
    "   - Leverage PyArrow's zero-copy reads\n",
    "\n",
    "4. **Data Quality**\n",
    "   - Validate files after writing\n",
    "   - Store metadata for data lineage\n",
    "   - Implement schema validation in pipelines\n",
    "\n",
    "### When to Use These Techniques:\n",
    "\n",
    "- **Schema Evolution**: When your data model changes over time\n",
    "- **Batch Processing**: For files larger than available RAM\n",
    "- **Row Group Filtering**: When you need specific data ranges\n",
    "- **Dictionary Encoding**: For columns with repeated values\n",
    "- **Type Optimization**: To reduce storage and improve performance\n",
    "\n",
    "### Next Steps:\n",
    "- Explore Apache Arrow Dataset API for multi-file datasets\n",
    "- Integrate with Apache Spark for distributed processing\n",
    "- Implement automated data quality pipelines\n",
    "- Learn about Delta Lake and Apache Iceberg for ACID transactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}