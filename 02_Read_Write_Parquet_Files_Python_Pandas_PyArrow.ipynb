{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Read and Write Parquet Files in Python: Complete Guide with Pandas and PyArrow\n",
    "\n",
    "This comprehensive tutorial covers everything you need to know about reading and writing Parquet files in Python using pandas and PyArrow. Learn best practices, performance optimization, and advanced techniques.\n",
    "\n",
    "## Table of Contents\n",
    "1. Creating Sample Data for Examples\n",
    "2. Writing Parquet Files with Pandas\n",
    "3. Writing Parquet Files with PyArrow\n",
    "4. Reading Parquet Files Efficiently\n",
    "5. Advanced Reading Techniques\n",
    "6. Working with Partitioned Datasets\n",
    "7. Cloud Storage Integration\n",
    "8. Best Practices and Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Sample E-commerce Dataset for Examples\n",
    "\n",
    "Let's create a realistic e-commerce dataset to demonstrate various Parquet operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample e-commerce data\n",
    "np.random.seed(42)\n",
    "n_records = 50000\n",
    "\n",
    "# Date range for orders\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2024, 1, 1)\n",
    "\n",
    "# Create dataset\n",
    "ecommerce_data = {\n",
    "    'order_id': [f'ORD-{i:06d}' for i in range(n_records)],\n",
    "    'customer_id': np.random.randint(1000, 5000, n_records),\n",
    "    'order_date': pd.date_range(start_date, end_date, periods=n_records),\n",
    "    'product_category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports'], n_records),\n",
    "    'product_name': [f'Product_{i % 1000}' for i in range(n_records)],\n",
    "    'quantity': np.random.randint(1, 10, n_records),\n",
    "    'unit_price': np.round(np.random.uniform(10, 500, n_records), 2),\n",
    "    'discount_percent': np.random.choice([0, 5, 10, 15, 20, 25], n_records),\n",
    "    'shipping_country': np.random.choice(['USA', 'UK', 'Germany', 'France', 'Italy', 'Spain'], n_records),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'PayPal', 'Bank Transfer', 'Cash'], n_records),\n",
    "    'is_premium_customer': np.random.choice([True, False], n_records, p=[0.3, 0.7]),\n",
    "    'customer_rating': np.random.choice([1, 2, 3, 4, 5], n_records, p=[0.05, 0.1, 0.2, 0.35, 0.3])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ecommerce_data)\n",
    "\n",
    "# Calculate total amount\n",
    "df['total_amount'] = df['quantity'] * df['unit_price'] * (1 - df['discount_percent']/100)\n",
    "\n",
    "print(\"E-commerce Dataset Created:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn Information:\")\n",
    "df.info()\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to Write Parquet Files with Pandas\n",
    "\n",
    "### Basic Write Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Basic write with default settings\n",
    "df.to_parquet('ecommerce_basic.parquet')\n",
    "\n",
    "# Method 2: Specify PyArrow engine explicitly\n",
    "df.to_parquet('ecommerce_pyarrow.parquet', engine='pyarrow')\n",
    "\n",
    "# Method 3: Write without index\n",
    "df.to_parquet('ecommerce_no_index.parquet', index=False)\n",
    "\n",
    "# Method 4: Write with specific compression\n",
    "df.to_parquet('ecommerce_compressed.parquet', compression='snappy', index=False)\n",
    "\n",
    "print(\"✅ Parquet files created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression Options Comparison\n",
    "\n",
    "Different compression algorithms offer various trade-offs between file size and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different compression methods\n",
    "compression_methods = ['snappy', 'gzip', 'brotli', 'lz4', 'zstd', None]\n",
    "compression_stats = []\n",
    "\n",
    "for compression in compression_methods:\n",
    "    filename = f'test_{compression if compression else \"uncompressed\"}.parquet'\n",
    "    \n",
    "    # Measure write time\n",
    "    start = time.time()\n",
    "    df.to_parquet(filename, compression=compression)\n",
    "    write_time = time.time() - start\n",
    "    \n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(filename) / (1024 * 1024)  # MB\n",
    "    \n",
    "    # Measure read time\n",
    "    start = time.time()\n",
    "    _ = pd.read_parquet(filename)\n",
    "    read_time = time.time() - start\n",
    "    \n",
    "    compression_stats.append({\n",
    "        'compression': compression if compression else 'none',\n",
    "        'file_size_mb': round(file_size, 2),\n",
    "        'write_time_s': round(write_time, 3),\n",
    "        'read_time_s': round(read_time, 3),\n",
    "        'compression_ratio': round((df.memory_usage(deep=True).sum() / (1024**2)) / file_size, 2)\n",
    "    })\n",
    "    \n",
    "    # Clean up\n",
    "    os.remove(filename)\n",
    "\n",
    "compression_df = pd.DataFrame(compression_stats)\n",
    "print(\"Compression Methods Performance Comparison:\")\n",
    "compression_df.sort_values('file_size_mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to Write Parquet Files with PyArrow (Advanced)\n",
    "\n",
    "PyArrow provides more control over the writing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrame to PyArrow Table\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Write with custom options\n",
    "pq.write_table(\n",
    "    table,\n",
    "    'ecommerce_custom.parquet',\n",
    "    compression='snappy',\n",
    "    use_dictionary=True,  # Enable dictionary encoding for strings\n",
    "    compression_level=None,  # Use default compression level\n",
    "    use_byte_stream_split=False,  # For floating point data\n",
    "    column_encoding='PLAIN',  # Encoding type\n",
    "    row_group_size=20000,  # Rows per row group\n",
    "    data_page_size=1024*1024  # 1MB data pages\n",
    ")\n",
    "\n",
    "print(\"✅ Custom Parquet file created with PyArrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write with custom schema\n",
    "# Define specific data types\n",
    "schema = pa.schema([\n",
    "    pa.field('order_id', pa.string()),\n",
    "    pa.field('customer_id', pa.int64()),\n",
    "    pa.field('order_date', pa.timestamp('ns')),\n",
    "    pa.field('product_category', pa.dictionary(pa.int32(), pa.string())),  # Dictionary encoding\n",
    "    pa.field('quantity', pa.int32()),\n",
    "    pa.field('unit_price', pa.float64()),\n",
    "    pa.field('total_amount', pa.float64()),\n",
    "    pa.field('is_premium_customer', pa.bool_())\n",
    "])\n",
    "\n",
    "# Select columns and create table with schema\n",
    "selected_columns = ['order_id', 'customer_id', 'order_date', 'product_category', \n",
    "                   'quantity', 'unit_price', 'total_amount', 'is_premium_customer']\n",
    "subset_df = df[selected_columns]\n",
    "table_with_schema = pa.Table.from_pandas(subset_df, schema=schema)\n",
    "\n",
    "# Write with schema\n",
    "pq.write_table(table_with_schema, 'ecommerce_with_schema.parquet')\n",
    "print(\"✅ Parquet file with custom schema created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How to Read Parquet Files in Python\n",
    "\n",
    "### Basic Read Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a reference file for reading examples\n",
    "df.to_parquet('ecommerce_data.parquet', compression='snappy')\n",
    "\n",
    "# Method 1: Basic read with pandas\n",
    "df_read = pd.read_parquet('ecommerce_data.parquet')\n",
    "print(f\"✅ DataFrame loaded: {df_read.shape}\")\n",
    "\n",
    "# Method 2: Read with specific engine\n",
    "df_pyarrow = pd.read_parquet('ecommerce_data.parquet', engine='pyarrow')\n",
    "print(f\"✅ Read with PyArrow: {df_pyarrow.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_read.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Specific Columns (Column Pruning)\n",
    "\n",
    "One of Parquet's key advantages is the ability to read only the columns you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only specific columns\n",
    "columns_to_read = ['order_id', 'customer_id', 'order_date', 'total_amount']\n",
    "df_subset = pd.read_parquet('ecommerce_data.parquet', columns=columns_to_read)\n",
    "\n",
    "print(f\"✅ Columns read: {df_subset.columns.tolist()}\")\n",
    "print(f\"Shape: {df_subset.shape}\")\n",
    "\n",
    "# Performance comparison: reading all vs specific columns\n",
    "start = time.time()\n",
    "_ = pd.read_parquet('ecommerce_data.parquet')\n",
    "time_all = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = pd.read_parquet('ecommerce_data.parquet', columns=columns_to_read)\n",
    "time_subset = time.time() - start\n",
    "\n",
    "print(f\"\\nPerformance improvement: {time_all/time_subset:.2f}x faster when reading only 4 columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Reading: Filters and Predicates\n",
    "\n",
    "PyArrow supports predicate pushdown, allowing you to filter data while reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 1: Simple equality filter\n",
    "filters = [('product_category', '==', 'Electronics')]\n",
    "df_electronics = pd.read_parquet('ecommerce_data.parquet', filters=filters)\n",
    "print(f\"Electronics orders: {len(df_electronics):,} rows\")\n",
    "\n",
    "# Filter 2: Multiple conditions (AND)\n",
    "filters = [\n",
    "    ('product_category', '==', 'Electronics'),\n",
    "    ('total_amount', '>', 1000)\n",
    "]\n",
    "df_expensive_electronics = pd.read_parquet('ecommerce_data.parquet', filters=filters)\n",
    "print(f\"Expensive electronics (>$1000): {len(df_expensive_electronics):,} rows\")\n",
    "\n",
    "# Filter 3: OR conditions (list of lists)\n",
    "filters = [\n",
    "    [('product_category', '==', 'Electronics')],\n",
    "    [('product_category', '==', 'Books')]\n",
    "]\n",
    "df_electronics_or_books = pd.read_parquet('ecommerce_data.parquet', filters=filters)\n",
    "print(f\"Electronics OR Books: {len(df_electronics_or_books):,} rows\")\n",
    "\n",
    "# Filter 4: Complex filters with different operators\n",
    "filters = [\n",
    "    ('customer_rating', '>=', 4),\n",
    "    ('is_premium_customer', '==', True),\n",
    "    ('shipping_country', 'in', ['USA', 'UK']),\n",
    "    ('order_date', '>=', pd.Timestamp('2023-06-01'))\n",
    "]\n",
    "df_premium_satisfied = pd.read_parquet('ecommerce_data.parquet', filters=filters)\n",
    "print(f\"Premium satisfied customers (rating>=4) in USA/UK after June 2023: {len(df_premium_satisfied):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading with PyArrow for More Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Parquet file with PyArrow\n",
    "parquet_file = pq.ParquetFile('ecommerce_data.parquet')\n",
    "\n",
    "# Get file metadata\n",
    "print(\"📊 File Metadata:\")\n",
    "print(f\"Total rows: {parquet_file.metadata.num_rows:,}\")\n",
    "print(f\"Number of row groups: {parquet_file.num_row_groups}\")\n",
    "print(f\"Created by: {parquet_file.metadata.created_by}\")\n",
    "\n",
    "# Schema information\n",
    "print(f\"\\n📋 Schema:\")\n",
    "for i, field in enumerate(parquet_file.schema):\n",
    "    print(f\"{i}: {field.name} - {field.physical_type}\")\n",
    "\n",
    "# Read specific row groups\n",
    "first_row_group = parquet_file.read_row_group(0)\n",
    "print(f\"\\n📦 First row group: {first_row_group.num_rows:,} rows\")\n",
    "\n",
    "# Read with filters using PyArrow\n",
    "table = pq.read_table('ecommerce_data.parquet',\n",
    "                     filters=[('total_amount', '>', 2000)],\n",
    "                     columns=['order_id', 'customer_id', 'total_amount'])\n",
    "df_high_value = table.to_pandas()\n",
    "print(f\"\\n💰 High value orders (>$2000): {len(df_high_value):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Working with Partitioned Parquet Datasets\n",
    "\n",
    "Partitioning is crucial for handling large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create partitioned dataset by product category\n",
    "partitioned_path = 'ecommerce_partitioned'\n",
    "\n",
    "# Write partitioned data\n",
    "df.to_parquet(\n",
    "    partitioned_path,\n",
    "    partition_cols=['product_category'],\n",
    "    engine='pyarrow',\n",
    "    compression='snappy'\n",
    ")\n",
    "\n",
    "# Display partition structure\n",
    "print(\"📁 Partition Structure:\")\n",
    "for root, dirs, files in os.walk(partitioned_path):\n",
    "    level = root.replace(partitioned_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "            file_size = os.path.getsize(os.path.join(root, file)) / 1024  # KB\n",
    "            print(f\"{subindent}{file} ({file_size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partitioned data\n",
    "# Read all partitions\n",
    "df_all_partitions = pd.read_parquet(partitioned_path)\n",
    "print(f\"All partitions: {df_all_partitions.shape}\")\n",
    "\n",
    "# Read specific partition\n",
    "df_electronics_partition = pd.read_parquet(f'{partitioned_path}/product_category=Electronics')\n",
    "print(f\"Electronics partition only: {df_electronics_partition.shape}\")\n",
    "print(f\"Note: partition column not included in file\")\n",
    "\n",
    "# Read with filters on partitioned data (very efficient!)\n",
    "df_filtered_partition = pd.read_parquet(\n",
    "    partitioned_path,\n",
    "    filters=[('product_category', 'in', ['Electronics', 'Books'])]\n",
    ")\n",
    "print(f\"\\nFiltered partitions (Electronics & Books): {df_filtered_partition.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Appending Data to Existing Parquet Files\n",
    "\n",
    "While Parquet files are immutable, you can append data by reading and rewriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data to append\n",
    "new_orders = pd.DataFrame({\n",
    "    'order_id': [f'ORD-NEW-{i:04d}' for i in range(1000)],\n",
    "    'customer_id': np.random.randint(1000, 5000, 1000),\n",
    "    'order_date': pd.date_range('2024-01-01', periods=1000, freq='H'),\n",
    "    'product_category': np.random.choice(['Electronics', 'Clothing', 'Books'], 1000),\n",
    "    'product_name': [f'NewProduct_{i}' for i in range(1000)],\n",
    "    'quantity': np.random.randint(1, 5, 1000),\n",
    "    'unit_price': np.round(np.random.uniform(20, 300, 1000), 2),\n",
    "    'discount_percent': np.random.choice([0, 10, 20], 1000),\n",
    "    'shipping_country': np.random.choice(['USA', 'UK'], 1000),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'PayPal'], 1000),\n",
    "    'is_premium_customer': np.random.choice([True, False], 1000),\n",
    "    'customer_rating': np.random.choice([3, 4, 5], 1000)\n",
    "})\n",
    "new_orders['total_amount'] = new_orders['quantity'] * new_orders['unit_price'] * (1 - new_orders['discount_percent']/100)\n",
    "\n",
    "# Method 1: Read, concatenate, and rewrite\n",
    "existing_df = pd.read_parquet('ecommerce_data.parquet')\n",
    "combined_df = pd.concat([existing_df, new_orders], ignore_index=True)\n",
    "combined_df.to_parquet('ecommerce_data_updated.parquet')\n",
    "\n",
    "print(f\"✅ Original file: {len(existing_df):,} rows\")\n",
    "print(f\"✅ New data: {len(new_orders):,} rows\")\n",
    "print(f\"✅ Updated file: {len(combined_df):,} rows\")\n",
    "\n",
    "# Method 2: For large files, use PyArrow dataset API\n",
    "# This is more memory efficient for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cloud Storage Integration\n",
    "\n",
    "Parquet works seamlessly with cloud storage services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of cloud storage URLs (requires appropriate credentials)\n",
    "\n",
    "# Amazon S3\n",
    "# df = pd.read_parquet('s3://my-bucket/path/to/file.parquet')\n",
    "# df.to_parquet('s3://my-bucket/path/to/output.parquet')\n",
    "\n",
    "# Azure Blob Storage\n",
    "# df = pd.read_parquet('abfs://container@account.dfs.core.windows.net/path/to/file.parquet')\n",
    "\n",
    "# Google Cloud Storage\n",
    "# df = pd.read_parquet('gs://my-bucket/path/to/file.parquet')\n",
    "\n",
    "print(\"☁️ Cloud Storage Integration:\")\n",
    "print(\"1. Install cloud SDK: boto3 (AWS), azure-storage-blob (Azure), google-cloud-storage (GCS)\")\n",
    "print(\"2. Configure credentials (AWS CLI, Azure CLI, gcloud)\")\n",
    "print(\"3. Use appropriate URL format: s3://, abfs://, gs://\")\n",
    "print(\"4. Pandas/PyArrow handles the rest automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Performance Tips\n",
    "\n",
    "### 1. Compression Guidelines\n",
    "- **Snappy**: Best balance of speed and compression (default)\n",
    "- **Gzip**: Better compression, slower\n",
    "- **Brotli/Zstd**: Maximum compression for archival\n",
    "- **LZ4**: Fastest compression/decompression\n",
    "\n",
    "### 2. Performance Optimization\n",
    "- Use column pruning - read only needed columns\n",
    "- Apply filters during read when possible\n",
    "- Partition by frequently filtered columns\n",
    "- Set appropriate row_group_size (default ~128MB)\n",
    "\n",
    "### 3. Schema Best Practices\n",
    "- Use dictionary encoding for categorical data\n",
    "- Consider int32 vs int64 for memory efficiency\n",
    "- Preserve data types to avoid conversion overhead\n",
    "\n",
    "### 4. Large Dataset Guidelines\n",
    "- Use partitioning for datasets > 1GB\n",
    "- Consider PyArrow Dataset API for very large files\n",
    "- Process data in chunks when memory is limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all temporary files\n",
    "import shutil\n",
    "\n",
    "files_to_remove = [\n",
    "    'ecommerce_basic.parquet', 'ecommerce_pyarrow.parquet', \n",
    "    'ecommerce_no_index.parquet', 'ecommerce_compressed.parquet',\n",
    "    'ecommerce_custom.parquet', 'ecommerce_with_schema.parquet',\n",
    "    'ecommerce_data.parquet', 'ecommerce_data_updated.parquet'\n",
    "]\n",
    "\n",
    "for file in files_to_remove:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "# Remove partitioned directory\n",
    "if os.path.exists('ecommerce_partitioned'):\n",
    "    shutil.rmtree('ecommerce_partitioned')\n",
    "\n",
    "print(\"✅ All temporary files removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this comprehensive guide, you learned:\n",
    "\n",
    "1. **Writing Parquet files** with pandas and PyArrow\n",
    "2. **Compression options** and their trade-offs\n",
    "3. **Reading efficiently** with column selection and filters\n",
    "4. **Partitioning strategies** for large datasets\n",
    "5. **Cloud storage integration** patterns\n",
    "6. **Best practices** for optimal performance\n",
    "\n",
    "### Next Steps:\n",
    "- Explore PyArrow Dataset API for multi-file datasets\n",
    "- Learn about schema evolution and compatibility\n",
    "- Integrate with big data tools like Spark and Dask\n",
    "- Optimize for specific use cases (OLAP vs OLTP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}