{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide to Apache Parquet Files in Python: Tutorial with Examples\n",
    "\n",
    "## What is Apache Parquet? A Comprehensive Introduction\n",
    "\n",
    "Apache Parquet is an open-source **columnar storage file format** optimized for use with big data processing frameworks. This tutorial will teach you everything you need to know about working with Parquet files in Python using pandas and PyArrow.\n",
    "\n",
    "### Key Features of Parquet Files:\n",
    "- **Columnar Storage Format**: Data is organized by column rather than by row\n",
    "- **Efficient Compression**: Reduces storage space by 70-90% compared to CSV\n",
    "- **Schema Evolution**: Supports adding, removing, or modifying columns over time\n",
    "- **Performance Optimized**: Faster read/write operations for analytical queries\n",
    "- **Cross-Platform Compatibility**: Works with Spark, Pandas, Arrow, and more\n",
    "\n",
    "### Parquet vs CSV: Performance Comparison\n",
    "1. **File Size**: Parquet files are typically 70-90% smaller than CSV\n",
    "2. **Read Speed**: 5-10x faster for analytical queries\n",
    "3. **Data Type Preservation**: Maintains original data types without parsing\n",
    "4. **Complex Data Support**: Handles nested structures and arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Install Required Libraries for Parquet in Python\n",
    "\n",
    "To work with Parquet files in Python, you need:\n",
    "- **pandas**: Data manipulation and analysis library\n",
    "- **pyarrow**: Apache Arrow Python bindings for Parquet support\n",
    "- **fastparquet**: Alternative Parquet engine (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Parquet file handling\n",
    "# Run this cell if packages are not already installed\n",
    "# !pip install pandas pyarrow\n",
    "# Optional alternative engine: !pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Parquet operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Display library versions\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"PyArrow version: {pa.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet vs CSV: Real-World Performance Benchmark\n",
    "\n",
    "Let's create a sample dataset and compare Parquet and CSV formats in terms of file size, read/write speed, and data type preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset for benchmarking\n",
    "np.random.seed(42)\n",
    "\n",
    "n_rows = 100000\n",
    "sample_data = {\n",
    "    'id': range(n_rows),\n",
    "    'user_name': [f'User_{i}' for i in range(n_rows)],\n",
    "    'age': np.random.randint(18, 80, n_rows),\n",
    "    'salary': np.random.randint(20000, 150000, n_rows),\n",
    "    'department': np.random.choice(['IT', 'HR', 'Sales', 'Marketing', 'R&D'], n_rows),\n",
    "    'hire_date': pd.date_range('2010-01-01', periods=n_rows, freq='H'),\n",
    "    'is_active': np.random.choice([True, False], n_rows, p=[0.85, 0.15])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare file sizes: CSV vs Parquet\n",
    "csv_file = 'sample_data.csv'\n",
    "parquet_file = 'sample_data.parquet'\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Save as Parquet\n",
    "df.to_parquet(parquet_file, index=False)\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = os.path.getsize(csv_file) / (1024 * 1024)  # MB\n",
    "parquet_size = os.path.getsize(parquet_file) / (1024 * 1024)  # MB\n",
    "\n",
    "print(f\"CSV file size: {csv_size:.2f} MB\")\n",
    "print(f\"Parquet file size: {parquet_size:.2f} MB\")\n",
    "print(f\"Size reduction: {(1 - parquet_size/csv_size)*100:.1f}%\")\n",
    "print(f\"\\nParquet is {csv_size/parquet_size:.1f}x smaller than CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Performance Comparison: Parquet vs CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark read performance\n",
    "# CSV read time\n",
    "start_time = time.time()\n",
    "df_csv = pd.read_csv(csv_file)\n",
    "csv_read_time = time.time() - start_time\n",
    "\n",
    "# Parquet read time\n",
    "start_time = time.time()\n",
    "df_parquet = pd.read_parquet(parquet_file)\n",
    "parquet_read_time = time.time() - start_time\n",
    "\n",
    "print(f\"CSV read time: {csv_read_time:.3f} seconds\")\n",
    "print(f\"Parquet read time: {parquet_read_time:.3f} seconds\")\n",
    "print(f\"\\nParquet is {csv_read_time/parquet_read_time:.1f}x faster than CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Parquet File Structure and Metadata\n",
    "\n",
    "Parquet files have a sophisticated internal structure:\n",
    "- **Row Groups**: Large chunks of rows (typically thousands)\n",
    "- **Column Chunks**: Column data within each row group\n",
    "- **Pages**: Smallest unit of data within column chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Parquet file metadata\n",
    "parquet_file_obj = pq.ParquetFile(parquet_file)\n",
    "\n",
    "print(\"Parquet File Metadata:\")\n",
    "print(f\"Total rows: {parquet_file_obj.metadata.num_rows}\")\n",
    "print(f\"Number of row groups: {parquet_file_obj.num_row_groups}\")\n",
    "print(f\"\\nFile Schema:\")\n",
    "print(parquet_file_obj.schema)\n",
    "\n",
    "# Row group information\n",
    "for i in range(min(3, parquet_file_obj.num_row_groups)):\n",
    "    rg = parquet_file_obj.metadata.row_group(i)\n",
    "    print(f\"\\nRow group {i}: {rg.num_rows} rows, {rg.total_byte_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Preservation: Parquet vs CSV\n",
    "\n",
    "One major advantage of Parquet is automatic data type preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare data types after reading\n",
    "print(\"Original DataFrame Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nData Types from CSV (notice datetime and boolean changes):\")\n",
    "print(df_csv.dtypes)\n",
    "print(\"\\nData Types from Parquet (preserved correctly):\")\n",
    "print(df_parquet.dtypes)\n",
    "\n",
    "# CSV loses datetime and boolean type information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Different Compression Algorithms\n",
    "\n",
    "Parquet supports multiple compression algorithms. Let's compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different compression methods\n",
    "compression_methods = ['snappy', 'gzip', 'brotli', 'lz4', 'zstd']\n",
    "compression_results = []\n",
    "\n",
    "for compression in compression_methods:\n",
    "    filename = f'test_{compression}.parquet'\n",
    "    \n",
    "    # Write time\n",
    "    start = time.time()\n",
    "    df.to_parquet(filename, compression=compression)\n",
    "    write_time = time.time() - start\n",
    "    \n",
    "    # File size\n",
    "    file_size = os.path.getsize(filename) / (1024 * 1024)  # MB\n",
    "    \n",
    "    # Read time\n",
    "    start = time.time()\n",
    "    _ = pd.read_parquet(filename)\n",
    "    read_time = time.time() - start\n",
    "    \n",
    "    compression_results.append({\n",
    "        'compression': compression,\n",
    "        'file_size_mb': round(file_size, 2),\n",
    "        'write_time_s': round(write_time, 3),\n",
    "        'read_time_s': round(read_time, 3)\n",
    "    })\n",
    "    \n",
    "    os.remove(filename)\n",
    "\n",
    "compression_df = pd.DataFrame(compression_results)\n",
    "print(\"Compression Methods Comparison:\")\n",
    "compression_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove temporary files\n",
    "for file in [csv_file, parquet_file]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "print(\"Temporary files removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways: Why Use Parquet Files?\n",
    "\n",
    "1. **Storage Efficiency**: Parquet files are 70-90% smaller than CSV\n",
    "2. **Better Performance**: 5-10x faster read operations\n",
    "3. **Data Type Preservation**: No need to parse or infer data types\n",
    "4. **Compression Options**: Multiple algorithms for different use cases\n",
    "5. **Big Data Ready**: Optimized for distributed computing frameworks\n",
    "\n",
    "### Best Practices:\n",
    "- Use **snappy** compression for balanced size/speed\n",
    "- Use **brotli** or **zstd** for maximum compression\n",
    "- Choose Parquet for analytical workloads\n",
    "- Leverage column pruning for better performance\n",
    "\n",
    "### Next Steps:\n",
    "In the next tutorial, we'll explore advanced Parquet operations including:\n",
    "- Reading specific columns\n",
    "- Filtering data during read\n",
    "- Working with partitioned datasets\n",
    "- Cloud storage integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}